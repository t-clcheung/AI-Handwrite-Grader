{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Processing\n",
    "1. Backup grading result.\n",
    "2. Generate score report.\n",
    "3. Create individual scored pdf.\n",
    "4. Collect samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file = \"../data/TestScript.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_name = os.path.basename(pdf_file)\n",
    "file_name = os.path.splitext(file_name)[0]\n",
    "base_path = \"../marking_form/\" + file_name\n",
    "base_path_images = base_path + \"/images/\"\n",
    "base_path_annotations = base_path+\"/annotations/\"\n",
    "base_path_questions = base_path+\"/questions\"\n",
    "\n",
    "base_path_marked_images = base_path + \"/marked/images/\"\n",
    "base_path_marked_pdfs = base_path + \"/marked/pdf/\"\n",
    "base_path_marked_scripts = base_path + \"/marked/scripts/\"\n",
    "os.makedirs(base_path_marked_images, exist_ok=True)\n",
    "os.makedirs(base_path_marked_pdfs, exist_ok=True)\n",
    "os.makedirs(base_path_marked_scripts, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove version history\n",
    "Before you backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for path, currentDirectory, files in os.walk(base_path_questions):\n",
    "    for file in files:\n",
    "        if file.startswith(\"control-\") or file.startswith(\"mark-\"):\n",
    "            os.remove(os.path.join(path, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(base_path,\"zip\",base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Score Report\n",
    "\n",
    "You must visit the ID and Name pages to verify the values before generate the marksheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "\n",
    "pageToStudentId = {}\n",
    "\n",
    "# read base_path_annotations\n",
    "with open(os.path.join(base_path_annotations, \"annotations.json\")) as f:\n",
    "    data = json.load(f)\n",
    "    # data is a dict and get the number of items\n",
    "numberOfPage = len(data)\n",
    "\n",
    "with open(os.path.join(base_path_questions, \"ID\", \"mark.json\")) as f:\n",
    "    data = json.load(f)\n",
    "    for i in data:\n",
    "        pageToStudentId[str(int(i[\"id\"]))] = (\n",
    "            i[\"overridedMark\"] if i[\"overridedMark\"] != \"\" else i[\"mark\"]\n",
    "        )\n",
    "\n",
    "def getStudentId(page):\n",
    "    # search reverse for the student ID page.\n",
    "    for p in range(page, page - numberOfPage, -1):\n",
    "        if str(p) in pageToStudentId:\n",
    "            return pageToStudentId[str(p)]\n",
    "    print(colored(\"{} is not in pageToStudentId!\".format(page), \"red\"))\n",
    "    return None\n",
    "\n",
    "\n",
    "questionAndMarks = {}\n",
    "for path, currentDirectory, files in os.walk(base_path_questions):\n",
    "    for file in files:\n",
    "        if file == \"mark.json\":\n",
    "            question = path[len(base_path_questions) + 1 :]\n",
    "            f = open(os.path.join(path, file))\n",
    "            data = json.load(f)\n",
    "            marks = {}\n",
    "            for i in data:\n",
    "                studentId = getStudentId(int(i[\"id\"]))\n",
    "                marks[studentId] = (\n",
    "                    i[\"overridedMark\"] if i[\"overridedMark\"] != \"\" else i[\"mark\"]\n",
    "                )\n",
    "            questionAndMarks[question] = marks\n",
    "            f.close()\n",
    "marksDf = pd.DataFrame(questionAndMarks)\n",
    "marksDf = marksDf[\n",
    "    [\"ID\", \"NAME\", \"CLASS\"]\n",
    "    + [\n",
    "        col\n",
    "        for col in sorted(marksDf.columns)\n",
    "        if col != \"ID\" and col != \"NAME\" and col != \"CLASS\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "marksDf[\"Marks\"] = (\n",
    "    marksDf.loc[:, ~marksDf.columns.isin([\"ID\", \"NAME\", \"CLASS\"])]\n",
    "    .apply(pd.to_numeric)\n",
    "    .sum(axis=1)\n",
    ")\n",
    "print(marksDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Scored Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy raw images to marked folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "if os.path.exists(base_path_marked_images):\n",
    "    shutil.rmtree(base_path_marked_images)\n",
    "shutil.copytree(base_path_images, base_path_marked_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "annotations_path = base_path_annotations + \"annotations.json\"\n",
    "with open(annotations_path, \"r\") as f: \n",
    "    annotations = json.load(f)          \n",
    "\n",
    "#flatten annotations to list \n",
    "annotations_list = []\n",
    "for page in annotations:\n",
    "    for annotation in annotations[page]:\n",
    "        annotation[\"page\"] = int(page)\n",
    "        # x to left, y to top\n",
    "        annotation[\"left\"] = annotation[\"x\"]\n",
    "        annotation[\"top\"] = annotation[\"y\"]\n",
    "        annotation.pop(\"x\")\n",
    "        annotation.pop(\"y\")\n",
    "        annotations_list.append(annotation) \n",
    "annotations_list\n",
    "\n",
    "# convert annotations_list to dict with key with label\n",
    "annotations_dict = {}\n",
    "for annotation in annotations_list:\n",
    "    annotations_dict[annotation[\"label\"]] = annotation\n",
    "annotations_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studentIdToPage={}\n",
    "with open(os.path.join(base_path_questions, \"ID\", \"mark.json\")) as f:\n",
    "    data = json.load(f)\n",
    "    for i in data:\n",
    "        studentId = i[\"overridedMark\"] if i[\"overridedMark\"] != \"\" else i[\"mark\"]\n",
    "        studentIdToPage[studentId] = int(i[\"id\"])\n",
    "studentIdToPage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import display\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "\n",
    "# Covert marksDf to dict\n",
    "marksDf_list = marksDf.to_dict(orient=\"records\")\n",
    "\n",
    "#get the max total digits for the file names\n",
    "max_total_digits = len(str(len(os.listdir(os.path.join(base_path_marked_images)))))\n",
    "\n",
    "f = IntProgress(min=0, max=len(marksDf_list))  # instantiate the bar\n",
    "display(f)  # display the bar\n",
    "\n",
    "def getLeadingZeros(num):\n",
    "    no_of_digits = len(str(num))\n",
    "    count_leading = ''\n",
    "    for i in range(no_of_digits,max_total_digits):\n",
    "        count_leading += '0'\n",
    "    return count_leading;\n",
    "\n",
    "for student in marksDf_list:\n",
    "    print(student[\"ID\"])\n",
    "    if not student[\"ID\"]==\"nan\" and str(student[\"ID\"]).isnumeric():\n",
    "        first_page = studentIdToPage[student[\"ID\"]]\n",
    "        for annotation in annotations_dict:\n",
    "            value = student[annotation]\n",
    "            x = annotations_dict[annotation][\"left\"]\n",
    "            y = annotations_dict[annotation][\"top\"]\n",
    "            page = first_page + annotations_dict[annotation][\"page\"]\n",
    "\n",
    "            image_path = base_path_marked_images + getLeadingZeros(page) + str(page) + \".jpg\"\n",
    "            # print(value, x, y, imagePath)\n",
    "            img = cv2.imread(image_path)\n",
    "            cv2.putText(img, str(value), (x, y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "            cv2.imwrite(image_path, img)\n",
    "    f.value += 1      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def getLeadingZeros(num):\n",
    "    no_of_digits = len(str(num))\n",
    "    count_leading = ''\n",
    "    for i in range(no_of_digits,max_total_digits):\n",
    "        count_leading += '0'\n",
    "    return count_leading;\n",
    "\n",
    "for student in marksDf_list:\n",
    "    studentId = student[\"ID\"]\n",
    "    if str(studentId).isnumeric():\n",
    "        first_page = studentIdToPage[student[\"ID\"]]\n",
    "        last_page = first_page + numberOfPage - 1\n",
    "        print(studentId, first_page, last_page)\n",
    "        pdf_path = base_path_marked_pdfs + studentId + \".pdf\"\n",
    "\n",
    "        images = list(map(Image.open, [base_path_marked_images + getLeadingZeros(i) + str(i) + \".jpg\" for i in range(first_page, last_page + 1)]))\n",
    "        images[0].save(pdf_path, save_all=True, append_images=images[1:]) \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Script Sample\n",
    "\n",
    "5 set Samples:\n",
    "1. Combined scripts\n",
    "2. 3 Good, 3 Average, and 3 Weak.\n",
    "3. 5 Good, 5 Average, and 5 Weak.\n",
    "4. 3 Good, 3 Average, and 3 Weak above the passing mark.\n",
    "5. 5 Good, 5 Average, and 5 Weak above the passing mark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "passingMark = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF4 import PdfFileMerger\n",
    "\n",
    "writer = PdfFileMerger(strict=True)\n",
    "\n",
    "# merge all pdfs in base_path_marked_pdfs\n",
    "for path, currentDirectory, files in os.walk(base_path_marked_pdfs):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(path, file)\n",
    "            writer.append(pdf_path)\n",
    "writer.write(base_path_marked_scripts + \"all.pdf\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output successfully written to../marking_form/TestScript/marked/scripts/sampleOf3.pdf\n",
      "Output successfully written to../marking_form/TestScript/marked/scripts/sampleOf5.pdf\n",
      "Output successfully written to../marking_form/TestScript/marked/scripts/sampleOf3_only_pass.pdf\n",
      "Output successfully written to../marking_form/TestScript/marked/scripts/sampleOf5_only_pass.pdf\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from PyPDF4 import PdfFileMerger, PdfFileReader\n",
    "\n",
    "sampling = marksDf.sort_values(by=[\"Marks\"], ascending=False)[\"Marks\"]\n",
    "\n",
    "from_directory = os.path.join(os.getcwd(), \"..\", \"templates\", \"pdf\")\n",
    "\n",
    "goodPage = PdfFileReader(from_directory + \"/Good.pdf\")\n",
    "averagePage = PdfFileReader(from_directory + \"/Average.pdf\")\n",
    "weakPage = PdfFileReader(from_directory + \"/Weak.pdf\")\n",
    "\n",
    "def get_scripts_psf(df):\n",
    "    return list(map(lambda rowNumber: base_path_marked_pdfs + rowNumber + \".pdf\", [item for item in df.index if item is not None]))\n",
    "\n",
    "\n",
    "def take_sample(n, sampling, suffix=\"\"):\n",
    "    if len(sampling) < 3 * n:\n",
    "        n = int(len(sampling) / 3)\n",
    "    good = sampling.head(n)\n",
    "    weak = sampling.tail(n)\n",
    "    median = int(len(sampling) / 2)\n",
    "    take = int(n / 2)\n",
    "    average = sampling.iloc[median - take : median + take]\n",
    "\n",
    "    merger = PdfFileMerger()\n",
    "    merger.append(goodPage)\n",
    "    for pdf in get_scripts_psf(good):\n",
    "        merger.append(PdfFileReader(pdf))\n",
    "    merger.append(averagePage)\n",
    "    for pdf in get_scripts_psf(average):\n",
    "        merger.append(PdfFileReader(pdf))\n",
    "    merger.append(weakPage)\n",
    "    for pdf in get_scripts_psf(weak):\n",
    "        merger.append(PdfFileReader(pdf))\n",
    "    fileName = base_path_marked_scripts + \"sampleOf\" + str(n) + suffix + \".pdf\"\n",
    "    merger.write(open(fileName, \"wb\"))\n",
    "    print(\"Output successfully written to\" + fileName)\n",
    "    merger.close()\n",
    "\n",
    "\n",
    "take_sample(3, sampling)\n",
    "take_sample(5, sampling)\n",
    "\n",
    "sampling = sampling.where(lambda x: x > passingMark)\n",
    "take_sample(3, sampling, \"_only_pass\")\n",
    "take_sample(5, sampling, \"_only_pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save marksDf to excel\n",
    "marksDf.to_excel(base_path_marked_scripts + \"details_score_report.xlsx\", index=False)\n",
    "# save marksDf to excel but only show ID, NAME, CLASS, Marks\n",
    "marksDf[[\"ID\", \"NAME\", \"CLASS\", \"Marks\"]].to_excel(base_path_marked_scripts + \"score_report.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='../marking_form/TestScript/marked/scripts/../scripts.zip' target='_blank'>../marking_form/TestScript/marked/scripts/../scripts.zip</a><br>"
      ],
      "text/plain": [
       "/workspaces/AI-Handwrite-Grader/marking_form/TestScript/marked/scripts.zip"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink \n",
    "\n",
    "# zip base_path_marked_scripts folder\n",
    "script_zip = base_path_marked_scripts + \"../scripts\"\n",
    "shutil.make_archive(script_zip, \"zip\", base_path_marked_scripts)\n",
    "FileLink(script_zip + \".zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
